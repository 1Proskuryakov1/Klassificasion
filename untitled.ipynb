{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1Proskuryakov1/Klassificasion/blob/main/untitled.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RY7TQidGUGY"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/kaggle.json /content/\n",
        "\n",
        "# Создание папки для хранения Kaggle API ключа и перемещение kaggle.json\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Загрузка датасета с Kaggle\n",
        "!kaggle datasets download -d rmisra/news-category-dataset\n",
        "\n",
        "# Распаковка загруженного датасета\n",
        "!unzip news-category-dataset.zip\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from wordcloud import WordCloud\n",
        "from wordcloud import STOPWORDS\n",
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "path = '/content/News_Category_Dataset_v3.json'\n",
        "df = pd.read_json(path, lines=True)\n",
        "df.head()\n",
        "df.info()\n",
        "df.describe()\n",
        "df.isna().sum()\n",
        "\n",
        "for column in df.columns:\n",
        "    if df[column].dtype == 'object':\n",
        "        num_empty_descriptions = (df[column].apply(len) == 0).sum()\n",
        "        print(f\"Количество строк, где длина '{column}' равна нулю: {num_empty_descriptions}\")\n",
        "        empty_headlines = df[df['headline'].str.strip().str.len() == 0]\n",
        "        print(empty_headlines)\n",
        "        print(f\"Количество различных категорий: {df.category.nunique()}\\n\")\n",
        "        print(f\"Категории: {df['category'].unique()}\\n\")\n",
        "        print(\"Категория | Кол-во статей\")\n",
        "        print(df['category'].value_counts())\n",
        "\n",
        "df.category = df.category.replace({\"HEALTHY LIVING\": \"WELLNESS\",\n",
        "              \"QUEER VOICES\": \"GROUPS VOICES\",\n",
        "              \"BUSINESS\": \"BUSINESS & FINANCES\",\n",
        "              \"PARENTS\": \"PARENTING\",\n",
        "              \"BLACK VOICES\": \"GROUPS VOICES\",\n",
        "              \"THE WORLDPOST\": \"WORLD NEWS\",\n",
        "              \"STYLE\": \"STYLE & BEAUTY\",\n",
        "              \"GREEN\": \"ENVIRONMENT\",\n",
        "              \"TASTE\": \"FOOD & DRINK\",\n",
        "              \"WORLDPOST\": \"WORLD NEWS\",\n",
        "              \"SCIENCE\": \"SCIENCE & TECH\",\n",
        "              \"TECH\": \"SCIENCE & TECH\",\n",
        "              \"MONEY\": \"BUSINESS & FINANCES\",\n",
        "              \"ARTS\": \"ARTS & CULTURE\",\n",
        "              \"COLLEGE\": \"EDUCATION\",\n",
        "              \"LATINO VOICES\": \"GROUPS VOICES\",\n",
        "              \"CULTURE & ARTS\": \"ARTS & CULTURE\",\n",
        "              \"FIFTY\": \"OTHER\",\n",
        "              \"GOOD NEWS\": \"OTHER\"}\n",
        "            )\n",
        "print(f\"Количество различных категорий: {df.category.nunique()}\\n\")\n",
        "print(f\"Категории: {df['category'].unique()}\\n\")\n",
        "print(\"Категория | Кол-во статей\")\n",
        "print(df['category'].value_counts())\n",
        "\n",
        "# График распределения категорий\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.countplot(y=df['category'], order=df['category'].value_counts().index)\n",
        "plt.title('Распределение категорий новостей')\n",
        "plt.xlabel('Количество статей')\n",
        "plt.ylabel('Категория')\n",
        "plt.show()\n",
        "\n",
        "# Вычисление длины заголовков\n",
        "headline_lengths = df['headline'].apply(len)\n",
        "\n",
        "# Вычисление квантилей\n",
        "quantiles = headline_lengths.quantile([0.25, 0.5, 0.75])\n",
        "\n",
        "# Построение гистограммы\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.histplot(headline_lengths, bins=50, kde=True)\n",
        " # Добавление квантилей на график\n",
        "for quantile in quantiles:\n",
        "    plt.axvline(quantile, color='r', linestyle='--')\n",
        "    # plt.text(quantile, plt.gca().get_ylim()[1]*0.9, f'{quantile:.2f}', color='r', ha='center')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plot_length_distribution(data, title, xlabel):\n",
        "    \"\"\"Строит гистограмму распределения длин текста с указанием квартилей.\"\"\"\n",
        "    lengths = data.apply(len)\n",
        "    quantiles = lengths.quantile([0.25, 0.5, 0.75])\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.histplot(lengths, bins=50, kde=True)\n",
        "\n",
        "    for quantile in quantiles:\n",
        "        plt.axvline(quantile, color='r', linestyle='--')\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel('Количество')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Настройки графика\n",
        "plt.title('Распределение длины описаний')\n",
        "plt.xlabel('Длина описания (кол-во символов)')\n",
        "plt.ylabel('Количество')\n",
        "plt.show()\n",
        "\n",
        "from matplotlib.cm import get_cmap\n",
        "\n",
        "# Преобразование даты в формат datetime и создание столбца с годом и месяцем\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df['year_month'] = df['date'].dt.to_period('M')\n",
        "\n",
        "categories = df['category'].unique()\n",
        "cmap = get_cmap('tab20', len(categories))\n",
        "\n",
        "plt.figure(figsize=(16,10))\n",
        "for i, category in enumerate(df['category'].unique()):\n",
        "    subset = df[df['category'] == category]\n",
        "    subset.groupby('year_month').size().plot(label=category, color=cmap(i))\n",
        "\n",
        "# Получение всех уникальных значений year_month\n",
        "all_year_months = df['year_month'].sort_values().unique()\n",
        "\n",
        "# x_labels = [all_year_months[0], all_year_months[len(all_year_months) // 2], all_year_months[-1]]\n",
        "num_labels = 10\n",
        "x_labels = [all_year_months[i] for i in range(0, len(all_year_months), len(all_year_months) // num_labels)]\n",
        "\n",
        "\n",
        "plt.title('Тренды новостей по категориям во времени', size=20)\n",
        "plt.xlabel('Дата')\n",
        "plt.ylabel('Количество новостей')\n",
        "plt.xticks(x_labels, [label.strftime('%Y-%m') for label in x_labels], rotation=45)\n",
        "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "categories = df['category'].unique()\n",
        "\n",
        "df['year_month'] = df['date'].dt.to_period('M').astype(str)\n",
        "\n",
        "figure, axis = plt.subplots(9, 3, figsize=(12, 15))\n",
        "\n",
        "for i, category in enumerate(categories):\n",
        "    # Получение тренда для текущей категории\n",
        "    trend = df[df['category'] == category].groupby('year_month').size()\n",
        "\n",
        "    row = i // 3\n",
        "    col = i % 3\n",
        "\n",
        "    # Построение графика для текущей категории\n",
        "    axis[row, col].plot(trend)\n",
        "    axis[row, col].set_title(category)\n",
        "\n",
        "    x_labels = [trend.index[0], trend.index[len(trend) // 2], trend.index[-1]]\n",
        "    axis[row, col].set_xticks(x_labels)\n",
        "    axis[row, col].set_xticklabels(x_labels)\n",
        "\n",
        "# Увеличение пространства между графиками\n",
        "plt.subplots_adjust(hspace=0.75)\n",
        "plt.show()\n",
        "\n",
        "# Фильтрация данных для новостей до 2018 года\n",
        "df_before_2018 = df[df['date'] < '2018-01-01']\n",
        "\n",
        "# Создание сетки подграфиков\n",
        "figure, axis = plt.subplots(9, 3, figsize=(12, 15))\n",
        "\n",
        "for i, category in enumerate(categories):\n",
        "    # Получение тренда для текущей категории и отфильтрованных данных\n",
        "    trend = df_before_2018[df_before_2018['category'] == category].groupby('year_month').size()\n",
        "\n",
        "    row = i // 3\n",
        "    col = i % 3\n",
        "\n",
        "    # Построение графика для текущей категории\n",
        "    axis[row, col].plot(trend)\n",
        "    axis[row, col].set_title(category)\n",
        "    try:\n",
        "        x_labels = [trend.index[0], trend.index[len(trend) // 2], trend.index[-1]]\n",
        "        axis[row, col].set_xticks(x_labels)\n",
        "        axis[row, col].set_xticklabels(x_labels)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "# Увеличение пространства между графиками\n",
        "plt.subplots_adjust(hspace=0.75)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Преобразование даты в формат datetime\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "# Создание нового столбца с днем недели\n",
        "df['day_of_week'] = df['date'].dt.day_name()\n",
        "\n",
        "# Группировка данных по категориям и дням недели и подсчет количества публикаций\n",
        "grouped_data = df.groupby(['category', 'day_of_week']).size().unstack()\n",
        "\n",
        "# Создание графика для каждой категории\n",
        "fig, axes = plt.subplots(9, 3, figsize=(12, 15))\n",
        "\n",
        "for ax, (category, data) in zip(axes.flatten(), grouped_data.iterrows()):\n",
        "    data = data.reindex(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
        "    data.plot(kind='bar', ax=ax, color='skyblue')\n",
        "    ax.set_title(category)\n",
        "    ax.set_xticklabels(data.index.str[0], rotation=0)\n",
        "    ax.set_xlabel(None)\n",
        "\n",
        "# Увеличение пространства между графиками\n",
        "plt.subplots_adjust(wspace=0.25, hspace=0.75)\n",
        "\n",
        "plt.suptitle('Частота публикаций по дням недели для различных категорий', fontsize=20)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Подсчет количества статей для каждого автора\n",
        "author_counts = df['authors'].value_counts()\n",
        "\n",
        "# Определение топ-10 самых активных авторов\n",
        "top_10_authors = author_counts[1:11]\n",
        "\n",
        "# Построение столбчатой диаграммы для топ-10 самых активных авторов\n",
        "plt.figure(figsize=(12, 6))\n",
        "top_10_authors.plot(kind='bar', color='skyblue')\n",
        "plt.title('Количество статей топ-10 самых активных авторов')\n",
        "plt.xlabel('Авторы')\n",
        "plt.ylabel('Количество статей')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "wc = WordCloud(max_words=1000,\n",
        "               min_font_size=10,\n",
        "               height=600,\n",
        "               width=1600,\n",
        "               background_color='black',\n",
        "               contour_color='black',\n",
        "               colormap='plasma',\n",
        "               repeat=False,\n",
        "               stopwords=STOPWORDS).generate(' '.join(df.headline))\n",
        "\n",
        "plt.title(\"Облако слов из всех заголовков\", size=15)\n",
        "plt.imshow(wc, interpolation= \"bilinear\")\n",
        "plt.axis('off')\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "wc = WordCloud(max_words=1000,\n",
        "               min_font_size=10,\n",
        "               height=600,\n",
        "               width=1600,\n",
        "               background_color='black',\n",
        "               contour_color='black',\n",
        "               colormap='plasma',\n",
        "               repeat=False,\n",
        "               stopwords=STOPWORDS).generate(' '.join(df.short_description))\n",
        "\n",
        "plt.title(\"Облако слов из всех описаний\", size=15)\n",
        "plt.imshow(wc, interpolation= \"bilinear\")\n",
        "plt.axis('off')\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(9, 3, figsize=(8, 16), subplot_kw=dict(xticks=[], yticks=[], frame_on=False))\n",
        "plt.subplots_adjust(hspace=0.35)\n",
        "for ax, category in zip(axes.flatten(), df['category'].unique()):\n",
        "    wordcloud = WordCloud(width=500, height=300, random_state=42, max_font_size=100, background_color='black',\n",
        "                         colormap='plasma', stopwords=STOPWORDS).generate(' '.join(df[df['category']==category]['headline']))\n",
        "    ax.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    ax.set_title(category)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(9, 3, figsize=(8, 15), subplot_kw=dict(xticks=[], yticks=[], frame_on=False))\n",
        "plt.subplots_adjust(hspace=0.35)\n",
        "for ax, category in zip(axes.flatten(), df['category'].unique()):\n",
        "    wordcloud = WordCloud(width=500, height=300, random_state=42, max_font_size=100, background_color='black',\n",
        "                         colormap='plasma', stopwords=STOPWORDS).generate(' '.join(df[df['category']==category]['short_description']))\n",
        "    ax.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    ax.set_title(category)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_ngrams(texts, n, num_top_ngrams, category):\n",
        "    \"\"\"\n",
        "    Plot the top N n-grams for the given texts.\n",
        "\n",
        "    Args:\n",
        "        texts (Series): Texts to analyze.\n",
        "        n (int): The n in n-grams (e.g., 1 for unigrams, 2 for bigrams).\n",
        "        num_top_ngrams (int): Number of top n-grams to display.\n",
        "        category (str): Category of the texts.\n",
        "    \"\"\"\n",
        "    vectorizer = CountVectorizer(ngram_range=(n, n))\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "    ngram_counts = X.sum(axis=0).A1\n",
        "\n",
        "    vocabulary = vectorizer.vocabulary_\n",
        "    ngrams = [gram for gram, idx in sorted(vocabulary.items(), key=lambda x: x[1])]\n",
        "\n",
        "    sorted_indices = ngram_counts.argsort()[::-1]\n",
        "    top_ngram_indices = sorted_indices[:num_top_ngrams]\n",
        "    top_ngrams = [ngrams[idx] for idx in top_ngram_indices]\n",
        "    top_ngram_counts = ngram_counts[top_ngram_indices]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(range(num_top_ngrams), top_ngram_counts, align='center', color='skyblue')\n",
        "    plt.yticks(range(num_top_ngrams), top_ngrams)\n",
        "    plt.xlabel('Count')\n",
        "    plt.ylabel('N-gram')\n",
        "    plt.title(f'Top {num_top_ngrams} {n}-grams - N-gram Analysis for {category} Category')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Пример использования для анализа топ 10 биграмм по категориям\n",
        "categories = df['category'].unique()\n",
        "for category in categories:\n",
        "    print(f\"N-gram Analysis for {category} Category:\")\n",
        "    category_texts = df[df['category'] == category]['headline']\n",
        "    plot_ngrams(category_texts, n=2, num_top_ngrams=10, category=category)\n",
        "    df['short_description'] = df['headline'] + df['short_description']\n",
        "    df = df[df['headline'] != '']\n",
        "    df.info()\n",
        "\n",
        "\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# Убедимся, что у нас есть необходимые данные NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Perform various preprocessing steps on the input text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to preprocess.\n",
        "\n",
        "    Returns:\n",
        "        str: The preprocessed text.\n",
        "    \"\"\"\n",
        "    # Удаление специальных символов и знаков (оставляем только буквы, цифры и пробелы)\n",
        "text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "\n",
        "# Приведение текста к нижнему регистру\n",
        "text = text.lower()\n",
        "\n",
        "# Удаление лишних пробелов (замена множества пробелов на один и обрезка пробелов по краям)\n",
        "text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "# Удаление стоп-слов (предварительно загруженных общеупотребительных слов английского языка)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "# Удаление не-алфавитно-цифровых символов (эквивалентно первому шагу, можно считать избыточным)\n",
        "text = re.sub(r'\\W+', ' ', text)\n",
        "\n",
        "# Удаление слов, начинающихся с хэштега (например, #пример)\n",
        "text = re.sub(r'\\b#\\w+\\b', '', text)\n",
        "\n",
        "# Удаление URL-адресов, начинающихся с http\n",
        "text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "# Лемматизация - приведение слов к их базовой форме\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = text.split()\n",
        "lemmas = [lemmatizer.lemmatize(word) for word in words]\n",
        "text = ' '.join(lemmas)\n",
        "\n",
        "# Удаление часто встречающихся слов, не несущих смысловой нагрузки (можно добавить свои)\n",
        "common_words = set(['news', 'article', 'report'])\n",
        "text = ' '.join([word for word in text.split() if word not in common_words])\n",
        "\n",
        "# Удаление цифр из текста\n",
        "text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "# Удаление коротких слов (менее 3 символов), которые обычно малоинформативны\n",
        "text = ' '.join([word for word in text.split() if len(word) >= 3])\n",
        "\n",
        "return text\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    \"\"\"\n",
        "    Convert a Penn Treebank POS tag to a WordNet POS tag.\n",
        "\n",
        "    Args:\n",
        "        treebank_tag (str): The Penn Treebank POS tag.\n",
        "\n",
        "    Returns:\n",
        "        str: The corresponding WordNet POS tag.\n",
        "    \"\"\"\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # Default to noun\n",
        "\n",
        "\n",
        "        # Удаление строк с пустыми заголовками или описаниями\n",
        "df = df[(df['headline'] != \"\") & (df['short_description'] != \"\")]\n",
        "\n",
        "# К описанию добавляем заголовок\n",
        "df['short_description'] = df['headline'] + \" \" + df['short_description']\n",
        "\n",
        "# Создание нового DataFrame для хранения выбранных записей\n",
        "sampled_df = pd.DataFrame()\n",
        "\n",
        "# Перебор всех уникальных категорий\n",
        "categories = df['category'].unique()\n",
        "\n",
        "for category in categories:\n",
        "    category_df = df[df['category'] == category]\n",
        "\n",
        "    # Если количество записей в категории меньше n, берем все записи, иначе берем n случайных записей\n",
        "    if len(category_df) < 500:\n",
        "        sampled_category_df = category_df\n",
        "    else:\n",
        "        sampled_category_df = category_df.sample(n=500, random_state=42)\n",
        "\n",
        "    # Добавляем выбранные записи в новый DataFrame\n",
        "    sampled_df = pd.concat([sampled_df, sampled_category_df], ignore_index=True)\n",
        "\n",
        "# Оставляем только нужные столбцы\n",
        "sampled_df = sampled_df[['short_description', 'category']]\n",
        "\n",
        "# Применение предобработки к столбцу short_description\n",
        "sampled_df['short_description'] = sampled_df['short_description'].apply(preprocess_text)\n",
        "\n",
        "# Энкодирование категорий\n",
        "label_encoder = LabelEncoder()\n",
        "sampled_df['category_encoded'] = label_encoder.fit_transform(sampled_df['category'])\n",
        "\n",
        "# Разделение данных на обучающую и тестовую выборки\n",
        "train_texts, eval_texts, train_labels, eval_labels = train_test_split(\n",
        "    sampled_df['short_description'].tolist(), sampled_df['category_encoded'].tolist(),\n",
        "    test_size=0.2,  # 80-20 разделение\n",
        "    random_state=42  # Для воспроизводимости\n",
        ")\n",
        "\n",
        "# Создание кастомного датасета\n",
        "class NewsCategoryDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]\n",
        "\n",
        "# Создание датасетов и загрузчиков данных\n",
        "train_dataset = NewsCategoryDataset(train_texts, train_labels)\n",
        "eval_dataset = NewsCategoryDataset(eval_texts, eval_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "# Инициализация устройства\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Working on {}\".format(device))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    spacy.require_gpu()  # Заставляет spaCy использовать GPU\n",
        "else:\n",
        "    print(\"GPU недоступен, будет использоваться CPU.\")\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # Загрузка модели spaCy\n",
        "\n",
        "def preprocess_text_spacy(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    doc = nlp(text.lower())\n",
        "    return \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "\n",
        "df = df[:5000]\n",
        "\n",
        "columns_to_process = ['headline', 'short_description']\n",
        "for col in columns_to_process:\n",
        "    df[col + '_lemmatized'] = df[col].apply(preprocess_text_spacy)\n",
        "\n",
        "print(df[['headline', 'headline_lemmatized', 'short_description', 'short_description_lemmatized']].head())\n",
        "df.sample(5)\n",
        "\n",
        "\n",
        "\n",
        "from transformers import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "num_labels = len(sampled_df['category'].unique())\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Функция для оценки модели\n",
        "def evaluate_model(eval_loader, model, device, label_encoder):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in eval_loader:\n",
        "            inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "            labels = torch.tensor(labels).to(device)\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            _, predicted = torch.max(outputs.logits, 1)\n",
        "\n",
        "            predictions.extend(predicted.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    print(f\"Accuracy on test set: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    cm = confusion_matrix(true_labels, predictions)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.show()\n",
        "\n",
        "    report = classification_report(true_labels, predictions, target_names=label_encoder.classes_)\n",
        "    print(report)\n",
        "\n",
        "# Обучение модели\n",
        "for epoch in range(7):  # Количество эпох обучения\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{7}\", leave=False)\n",
        "    for texts, labels in train_loader_tqdm:\n",
        "        inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        labels = torch.tensor(labels).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(**inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        train_loader_tqdm.set_postfix(loss=running_loss / len(train_loader_tqdm))\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "    # Оценка модели после каждой эпохи\n",
        "    evaluate_model(eval_loader, model, device, label_encoder)\n",
        "\n",
        "\n",
        "    # Сохранение обученной модели\n",
        "model_path = \"./saved_model\"\n",
        "model.save_pretrained(model_path)\n",
        "\n",
        "# Сохранение токенизатора\n",
        "tokenizer_path = \"./saved_tokenizer\"\n",
        "tokenizer.save_pretrained(tokenizer_path)\n",
        "\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/bert_finetuned_model/model\"\n",
        "tokenizer_path = \"/content/drive/MyDrive/bert_finetuned_model/tokenizer\"\n",
        "\n",
        "# Загрузка сохранённой модели\n",
        "model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# Загрузка сохранённого токенизатора\n",
        "tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Оценка модели на тестовой выборке\n",
        "model.eval()\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for texts, labels in eval_loader:\n",
        "        inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        labels = torch.tensor(labels).to(device)\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        _, predicted = torch.max(outputs.logits, 1)\n",
        "\n",
        "        predictions.extend(predicted.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "\n",
        "# Вычисление точности\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "print(f\"Accuracy on test set: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Матрица ошибок\n",
        "cm = confusion_matrix(true_labels, predictions)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n",
        "\n",
        "# Отчёт о классификации\n",
        "report = classification_report(true_labels, predictions, target_names=label_encoder.classes_)\n",
        "print(report)\n",
        "\n",
        "\n",
        "\n",
        "# Создание нового DataFrame для хранения выбранных записей\n",
        "user_df = pd.DataFrame()\n",
        "\n",
        "# Перебор всех уникальных категорий\n",
        "categories = df['category'].unique()\n",
        "\n",
        "for category in categories:\n",
        "    category_df = df[df['category'] == category]\n",
        "\n",
        "    # Если количество записей в категории меньше n, берем все записи, иначе берем n случайных записей\n",
        "    if len(category_df) < 250:\n",
        "        sampled_category_df = category_df\n",
        "    else:\n",
        "        sampled_category_df = category_df.sample(n=250)\n",
        "\n",
        "    # Добавляем выбранные записи в новый DataFrame\n",
        "    user_df = pd.concat([user_df, sampled_category_df], ignore_index=True)\n",
        "\n",
        "# user_df = df[df['category'] == \"POLITICS\"].sample(n=100, random_state=42)\n",
        "\n",
        "# Оставляем только нужные столбцы\n",
        "user_df = user_df[['short_description', 'category']]\n",
        "\n",
        "# Применение предобработки к столбцу short_description\n",
        "user_df['short_description'] = user_df['short_description'].apply(preprocess_text)\n",
        "\n",
        "# Энкодирование категорий\n",
        "label_encoder = LabelEncoder()\n",
        "user_df['category_encoded'] = label_encoder.fit_transform(user_df['category'])\n",
        "\n",
        "\n",
        "user_df.sample(5)\n",
        "\n",
        "\n",
        "# Создание кастомного датасета\n",
        "user_texts = user_df['short_description'].tolist()\n",
        "user_labels = user_df['category_encoded'].tolist()\n",
        "\n",
        "user_dataset = NewsCategoryDataset(user_texts, user_labels)\n",
        "user_loader = DataLoader(user_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "# Оценка модели на пользовательской выборке\n",
        "model.eval()\n",
        "user_predictions = []\n",
        "user_true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for texts, labels in user_loader:\n",
        "        inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        labels = torch.tensor(labels).to(device)\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        _, predicted = torch.max(outputs.logits, 1)\n",
        "\n",
        "        user_predictions.extend(predicted.cpu().numpy())\n",
        "        user_true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Вычисление точности\n",
        "user_accuracy = accuracy_score(user_true_labels, user_predictions)\n",
        "print(f\"Accuracy on user set: {user_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Матрица ошибок\n",
        "user_cm = confusion_matrix(user_true_labels, user_predictions)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(user_cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n",
        "\n",
        "# Отчёт о классификации\n",
        "user_report = classification_report(user_true_labels, user_predictions, target_names=label_encoder.classes_)\n",
        "print(user_report)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Функция для предсказания категории для нового текста\n",
        "def predict_category(text):\n",
        "    text = preprocess_text(text)\n",
        "    print(f\"Preprocessed text: {text}\")\n",
        "    model.eval()\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        _, predicted = torch.max(outputs.logits, 1)\n",
        "\n",
        "    predicted_category = label_encoder.inverse_transform(predicted.cpu().numpy())\n",
        "    return predicted_category[0]\n",
        "\n",
        "# Пример использования функции предсказания\n",
        "example_text = \"yummy sandwiches and cola are perfect for a lunch in sunny new york\"\n",
        "example = df.sample()\n",
        "# Извлекаем значение столбца \"short_description\" для этого примера\n",
        "# example_text = example['short_description'].values[0]\n",
        "print(\"Example text:\", example_text)\n",
        "predicted_category = predict_category(example_text)\n",
        "# real_category = example['category'].values[0]\n",
        "# print(f\"Real category: {real_category}\")\n",
        "print(f\"Predicted category: {predicted_category}\")\n",
        "\n",
        "import torch\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Пути к сохранённой модели и токенизатору\n",
        "model_path = \"/content/saved_model\"\n",
        "tokenizer_path = \"/content/saved_tokenizer\"\n",
        "\n",
        "# Загрузка сохранённой модели\n",
        "model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# Загрузка сохранённого токенизатора\n",
        "tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
        "\n",
        "# Проверка доступности устройства (GPU или CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Перемещение модели на нужное устройство\n",
        "model = model.to(device)\n",
        "\n",
        "# Оценка модели на тестовой выборке\n",
        "model.eval()\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for texts, labels in eval_loader:  # Убедитесь, что eval_loader загружает данные\n",
        "        # Токенизация текста\n",
        "        inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "        # Перемещение входных данных на нужное устройство\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        labels = labels.to(device)  # Убедитесь, что метки тоже на устройстве\n",
        "\n",
        "        # Прогон данных через модель\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # Получаем предсказания\n",
        "        _, predicted = torch.max(outputs.logits, 1)\n",
        "\n",
        "        # Сохранение предсказаний и истинных меток\n",
        "        predictions.extend(predicted.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Вычисление точности\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "print(f\"Accuracy on test set: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Генерация матрицы ошибок\n",
        "cm = confusion_matrix(true_labels, predictions)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n",
        "\n",
        "# Генерация отчета о классификации с закодированными метками\n",
        "report = classification_report(true_labels, predictions, target_names=label_encoder.classes_)\n",
        "print(report)\n",
        "\n",
        "\n",
        "# Создание нового DataFrame для хранения выбранных записей\n",
        "user_df = pd.DataFrame()\n",
        "\n",
        "# Перебор всех уникальных категорий\n",
        "categories = df['category'].unique()\n",
        "\n",
        "for category in categories:\n",
        "    category_df = df[df['category'] == category]\n",
        "\n",
        "    # Если количество записей в категории меньше n, берем все записи, иначе берем n случайных записей\n",
        "    if len(category_df) < 250:\n",
        "        sampled_category_df = category_df\n",
        "    else:\n",
        "        sampled_category_df = category_df.sample(n=250)\n",
        "\n",
        "    # Добавляем выбранные записи в новый DataFrame\n",
        "    user_df = pd.concat([user_df, sampled_category_df], ignore_index=True)\n",
        "\n",
        "# user_df = df[df['category'] == \"POLITICS\"].sample(n=100, random_state=42)\n",
        "\n",
        "# Оставляем только нужные столбцы\n",
        "user_df = user_df[['short_description', 'category']]\n",
        "\n",
        "# Применение предобработки к столбцу short_description\n",
        "user_df['short_description'] = user_df['short_description'].apply(preprocess_text)\n",
        "\n",
        "# Энкодирование категорий\n",
        "label_encoder = LabelEncoder()\n",
        "user_df['category_encoded'] = label_encoder.fit_transform(user_df['category'])\n",
        "\n",
        "\n",
        "user_df.sample(5)\n",
        "\n",
        "\n",
        "# Создание кастомного датасета\n",
        "user_texts = user_df['short_description'].tolist()\n",
        "user_labels = user_df['category_encoded'].tolist()\n",
        "\n",
        "user_dataset = NewsCategoryDataset(user_texts, user_labels)\n",
        "user_loader = DataLoader(user_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "# Оценка модели на пользовательской выборке\n",
        "model.eval()\n",
        "user_predictions = []\n",
        "user_true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for texts, labels in user_loader:\n",
        "        inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        labels = torch.tensor(labels).to(device)\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        _, predicted = torch.max(outputs.logits, 1)\n",
        "\n",
        "        user_predictions.extend(predicted.cpu().numpy())\n",
        "        user_true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Вычисление точности\n",
        "user_accuracy = accuracy_score(user_true_labels, user_predictions)\n",
        "print(f\"Accuracy on user set: {user_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Матрица ошибок\n",
        "user_cm = confusion_matrix(user_true_labels, user_predictions)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(user_cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Обучение LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(user_df['category'])  # Преобразуем категории в числовые значения\n",
        "\n",
        "# Получаем истинные метки и предсказания\n",
        "user_true_labels = label_encoder.transform(user_true_labels)  # Преобразуем метки в числовой формат\n",
        "user_predictions = label_encoder.transform(user_predictions)  # Преобразуем предсказания в числовой формат\n",
        "\n",
        "# Отчёт о классификации\n",
        "user_report = classification_report(user_true_labels, user_predictions, target_names=label_encoder.classes_)\n",
        "print(user_report)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Функция для предсказания категории для нового текста\n",
        "def predict_category(text):\n",
        "    text = preprocess_text(text)\n",
        "    print(f\"Preprocessed text: {text}\")\n",
        "    model.eval()\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        _, predicted = torch.max(outputs.logits, 1)\n",
        "\n",
        "    predicted_category = label_encoder.inverse_transform(predicted.cpu().numpy())\n",
        "    return predicted_category[0]\n",
        "\n",
        "# Пример использования функции предсказания\n",
        "example_text = \"yummy sandwiches and cola are perfect for a lunch in sunny new york\"\n",
        "example = df.sample()\n",
        "# Извлекаем значение столбца \"short_description\" для этого примера\n",
        "# example_text = example['short_description'].values[0]\n",
        "print(\"Example text:\", example_text)\n",
        "predicted_category = predict_category(example_text)\n",
        "# real_category = example['category'].values[0]\n",
        "# print(f\"Real category: {real_category}\")\n",
        "print(f\"Predicted category: {predicted_category}\")\n",
        "\n",
        "\n",
        "from fastapi import FastAPI\n",
        "\n",
        "app = FastAPI()\n",
        "@app.post(\"/classify\")\n",
        "def classify_news(text: str):\n",
        "    predicted_category = predict_category(text)\n",
        "    return {\"category\": predicted_category}\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "15tKyVrHPbIHE8orBvTYbzIIGWTpXPfRG",
      "authorship_tag": "ABX9TyPJvMcdPxUJCQ0seu4/j8wz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}